---
title: Ceph社区动态（2022-3-15~2022-4-15）
category: blog 
date: 2022-4-26
tags:
    - Ceph
    - 动态
    - Quincy
    - openEuler
    - ODD
sig: ceph-sig
archives: 2022-4
author: rosinL
summary: Ceph社区动态
---
# Ceph社区动态（2022-3-15~2022-4-15）
## openEuler社区Ceph进展
### ODD 2022(openEuler Developer Day 2022)存储相关议题
2022-04-13~2022-04-15，openEuler Developer Day 2022（简称[ODD 2022](https://www.openeuler.org/zh/interaction/summit-list/devday2022/)）在线上举行，ODD是开放原子开源基金会的 openEuler 社区发起的开发者大会。旨在推动 openEuler 在服务器、云计算、边缘计算和嵌入式四大场景的技术探索和创新。本次 ODD 2022 正式发布 openEuler 22.03 全场景长周期版本，展示社区伙伴联合创新成果，分享多行业使用 openEuler 规模部署的商业案例，举办社区治理机构的线上工作会议。  
openEuler Ceph版本紧跟Ceph原生社区，针对ARM架构开展优化，打造高质量、高可靠、高性能的Ceph版本。本次ODD大会与Ceph存储相关的议题有如下：
* 缓存策略对Ceph性能的影响 （XSKY 池信泽）
  - 主要是介绍CEPH RADOS架构下各级缓存策略对性能的影响
* 基于UADK加速器提升Ceph与大数据应用性能 (华为 戴志威)
  - 数据存储中的加密、压缩对CPU算力提出了更高的要求。UADK是鲲鹏CPU上基于SVA的用户态加速开发套件，支持加密、压缩的计算加速和卸载。介绍Ceph数据体系和压缩特性，以及UADK在Ceph和大数据应用上的适配进展，性能表现
* 大数据+Ceph存算分离、算子下推提升计算存储效率（华为 吴启庆）
  - 大数据+Ceph采用数据直通和算子下推方案，实现存算分离，提升计算存储效率，性能和存储率优于HDFS存储，成为数据湖实现的解决方案之一
* Arm SVE 向量指令为⾼性能计算库加速（Linaro 徐国栋，庄浩坚）
  - ⾯向⾼性能计算(HPC)和存储领域， Arm 推出了 SVE(Scalable Vector Extenstion)，⾄今演进有 SVE2 ，SME (armv9)。本演讲介绍 SVE 演进，⼯具链⽀持，软件适配状况。最后结合在 ISA-L ， XXHash 的编码，讲解SVE 的优势，同时介绍开发技术要点
### openEuler 22.09 Ceph相关版本规划
  * 在Ceph上使能鲲鹏加速器压缩和加密特性
  * 性能优化
  * Ceph国密支持
  * Crimson特性使能与评估
  * Ceph集群关机、开机自动化实现
  > 详情查看[2022年4月14日 sig-Ceph openEuler Developer Day 2022纪要](https://gitee.com/src-openeuler/ceph/wikis/%E4%BC%9A%E8%AE%AE%E7%BA%AA%E8%A6%81/2022%E5%8F%8C%E5%91%A8%E4%BE%8B%E4%BC%9A%E4%BC%9A%E8%AE%AE%E7%BA%AA%E8%A6%81)
## [Ceph v17.2.0 Quincy released](https://ceph.io/en/news/blog/2022/v17-2-0-quincy-released/)发布
相比于Pacific版本，有如下主要更新：
* General
  - FileStore已被弃用，BlueStore成为默认的ObjectStore
  - `device_health_metric pool`重命名为`.mgr`, 用作ceph-mgr模块的公共存储
  - `ceph pg dump`命令增加了3个新列：`LAST_SCRUB_DURATION`, `SCRUB_SCHEDULING`，`OBJECTS_SCRUBBED`
  - `WITH_LEVEL_DB`已被移除，全部迁移到RocksDB
  - `osd_memory_target_autotune`默认被开启，被设置为总内存的0.7
* Rados
  - OSD：使用`mclock_scheduler`作为BlueStore OSD的默认的`osd_op_queue`来提供Qos
  - MGR：`pg_autoscaler`现在可以使用`noautoscale`标志全局打开和关闭，默认设置为`on`
  - OSD：支持了osd-osd之间的通信压缩，默认为`off`
  - OSD：集群日志中`slow ops`的简明报告，通过将`osd_aggrregated_slow_ops_logging`设置为`false`可以恢复旧的和更详细的日志记录
* RBD
  - rbd-nbd：增加了`rbd device attch`和`rbd device detach`命令，允许在rbd-nbd守护进程重启之后可以安全的连接(kernel5.14)
  - rbd-nbd：增加了`notrim`映射选项，以支持**thick-provisioned images**
  - 针对SSD设备上的客户端持久缓存进行了大量稳定性修复工作
* RGW
  - RGW现在支持按照`user`和`bucket`进行限速，可以通过全局配置应用于所有的`user`和`bucket`
  - 修复S3 bucket通知事件的消息格式
  - 修复`Multipart upload`行为：仅在multipart upload完成之后发送一个CompleteMultipartUpload通知，Upload开始时的POST通知和每个part上发送的通知将不再发送
* CephFS
  - 可以使用特定的ID("fscid")创建文件系统，这对文件系统恢复场景非常有效（当Mon数据库丢失并重建时，恢复的文件系统会拥有和之前系统相同的ID）

## 近期社区合入pr
近期pr主要以bug修复为主，摘选了部分如下：
* BlueStore:
  - 在磁盘设备上，禁用NCB功能，NCB代码在OSD crash之后用于恢复allocation Map，但是由于HDD的性能比SSD慢了20倍，因此该功能在HDD环境下不适用，因此禁用该功能 [pr#45340](https://github.com/ceph/ceph/pull/45340)
  - 修复NCB SimpleBitMap边界检查的bug [pr#45733](https://github.com/ceph/ceph/pull/45733)
  - 重构bluefs中的get_block_extents接口 [pr#45250](https://github.com/ceph/ceph/pull/44250)
* crimson：
  - 修复由于bufferlist参数引用引起的OSD crash的bug [pr#45415](https://github.com/ceph/ceph/pull/45415)
  - 在vstart.sh中，针对crimson-osd禁用pg-autoscaling功能 [pr#45317](https://github.com/ceph/ceph/pull/45317)
  - 增加如下功能：cmp xattr cancel operation [pr#45774](https://github.com/ceph/ceph/pull/45774)，omap cmp [pr#45630](https://github.com/ceph/ceph/pull/45630)，osd_op_assert_ver [pr#45348](https://github.com/ceph/ceph/pull/45348)
* osd：
  - 修复快照删除中的bug：仅在scrub完成之后重新启动快照删除，修复CephFS场景下的，大量快照删除中PG状态不能恢复的bug [pr#45785](https://github.com/ceph/ceph/pull/45785)
  - sparse reads支持truncation sequnces [pr#45103](https://github.com/ceph/ceph/pull/45103)
* mon：
  - 修改`target_size_ratio`的范围从原来的`0.0->1.0`为`0.0->nolimit`，主要解决`ceph osd pool create <poolname> --target_size_ratio <ratio>`时传入的ratio大于1.0的错误 [pr#45078](https://github.com/ceph/ceph/pull/45078)
* 其他：
  - 主要包括dashboard和cephadm的一些bug修复
  - rbd和rgw的bug修复
  - 文档修复
## 近期Ceph Developer动态
Ceph社区各个模块会定期举行会议，讨论和对齐开发进展，会后有视频上传至[youtube](https://www.youtube.com/channel/UCno-Fry25FJ7B4RycCxOtfw/videos)，主要会议信息如下：
|会议名称|说明|频率|
|-------|----|----|
|Crimson SeaStore OSD Weekly Meeting |Crimson & Seastore开发周例会|周|
|Ceph Orchestration Meeting|Ceph管理模块（Mgr）开发|周|
|Ceph DocUBetter Meeting |文档优化|双周|
|Ceph Performance Meeting|Ceph性能优化|双周|
|Ceph Developer Monthly|Ceph开发者月度例会|月|
|Ceph Testing Meeting|版本验证及发布例会|月|
|Ceph Science User Group Meeting|Ceph科学计算领域会议|不定期|
|Ceph Leadership Team Meeting|Ceph领导团队例会|周|
|Ceph Tech talks|Ceph社区技术相关主题讨论|月|

### Performance Meeting
* 2022-03-17：[performace weekly](https://www.youtube.com/watch?v=RFYA-0k6QE)
  - Crimson cyanstore性能测试
  - rocksdb iterator性能讨论
  - PGLog优化点讨论
* 2022-03-24: [performace weekly](https://www.youtube.com/watch?v=tulzatiqqHo)
  - Quincy版本large write性能测试回归
  - RocksDB TombStone：在LSM中删除一个deleter是快速的，但是会使后续的查询变慢，尤其是大量的TombStone可能会妨碍查询，特别是范围查询
* 2022-03-31 [performace weekly](https://www.youtube.com/watch?v=NuofFc1539I) ~ 2022-04-07[performace weekly](https://www.youtube.com/watch?v=F34BvWvEDf4)
  - Quincy版本 写测试回归
  - 测试性能结果相比与Pacific16.2.7低，原因还在分析和讨论中
